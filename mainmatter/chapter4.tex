\chapter[Chapter 4: Evaluating Classroom Performance]{Evaluating Classroom Performance}

\section{Background}

\section{Exams and Quizzes}

\section{Homework}

\section{Concept Assessment Instruments}

Concept inventories and assessment instruments are a useful method of assessing student knowledge of the material; however, they are not simply tests that can be quickly put together and administered year after year. Lindell and Ding describe how it takes years to determine the validity and reliability of the results of a given concept inventory. They describe how reliability (precision) and validity (accuracy) play a role in the inventories and cite how factors such as age, course structure, geography, language, delivery of the tests, and wording of questions can influence the results of the assessment\cite{lindell2012}.

\subsection{Brief Electricity and Magnetism Assessment (BEMA)}

The Brief Electricity and Magnetism Assessment (􏰃BEMA)􏰄 was developed by Ruth Chabay, Bruce Sherwood, and Fred Reif in 1997. Although it was originally designed to measure student retention of electricity and magnetism concepts three months to five semesters after completing an introductory electricity and magnetism course, it is now often used to analyze student learning between the beginning and end of the semester. It is a useful tool to assess the understanding of electricity and magnetism concepts that are covered in a college-level calculus-based introductory physics course\cite{ding2006}.

The BEMA is a multiple choice test consisting of qualitative questions and a few simple calculations. Lin Ding et al. performed an analysis of the BEMA, showing that it is a reliable assessment tool for introductory electricity and magnetism courses\cite{ding2006}. We use the BEMA extensively in our research to analyze student's understanding of the course material.

\subsection{Force Concept Inventory}

The force concept inventory (􏰀FCI)􏰁 is a multiple choice test that is used to measure a student's understanding of introductory mechanics. It is given at the beginning of an introductory mechanics course as a pre-test and again at the end of the course as a post-test. The pool of answers on the test are designed to correspond to common student misconceptions of mechanics; they were developed through a series of student interviews\cite{hestenes1992}.

Coletta et al. have observed a strong correlation between the normalized gain on the FCI and SAT scores. They go so far as to state that SAT scores might be a good indicator of the expected normalized gains within a classroom\cite{coletta2007}.

\section{Other Methods}

\section{Numerical Measures}

\subsection{Hake Factor (Gain Index)}

Normalized gain (G) vs. normalized gain <g>

\subsection{Gender Gap}

Really focus on a research question for CITA
What am I trying to improve? What should I test?
Test scores -> Assess using PHYS 241 exam grades
Homework scores -> Assess using PHYS 241 homework grades
Quiz grades -> Assess using PHYS 241 quiz grades
Overall grades -> Assess using PHYS 241 course grades
Knowledge of qualitative electromagnetism concepts -> Assess using BEMA
Problem solving strategy -> Assess using worked out problems with custom rubrics
Student enjoyment in physics -> Personal interviews and class surveys
Student confidence in problem solving -> Personal interviews, worked out problems, and class surveys
Ability to make predictions in new and novel situations -> Give students advanced problems that they need to categorize
Ability to create diagrams -> Custom problems, draw a diagram from written directions
Ability to teach a concept -> Have them reteach the concept using a custom rubric
Close the gap between genders -> Test each gender and see if the gap closes
Close the gap between socioeconomic groups -> Test each group and see if the gap closes
Catch US students up to foreign students -> Test each group and see if the gap closes
Perform better in the lab (physical experiments) -> Create a lab assessment, maybe between 172 and 241
“Thinking like a physicist” -> Videotape how students solve interesting problems
The BEMA might not be a good way to assess CITA
Consider worked out problems
Track how far students get into problem using a custom rubric
Perhaps this can be implemented during recitation
Bruce and Ruth have a great presentation on their site as a demonstration